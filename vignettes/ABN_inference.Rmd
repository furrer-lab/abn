---
title: "Inference with ABN"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Inference with ABN}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This vignette provides an overview of performing inference with ABN.

## Background

In Additive Bayesian Networks, inference refers to the process of computating the expectation of a subset of variables, given observed or known values of other variables in the network.

**Example**: let's consider the following example. 

```{r, message=FALSE}
library(abn)

# Load an illustrative example
mydat <- ex1.dag.data

# Specify the distributions of the nodes
mydists <- list(b1="binomial", 
                p1="poisson",
                g1="gaussian", 
                b2="binomial", 
                p2="poisson",
                b3="binomial", 
                g2="gaussian", 
                b4="binomial", 
                b5="binomial", 
                g3="gaussian") 

# Run abn
max.par <- 4 # set the same max parents for all nodes
mycache <- buildScoreCache(data.df = mydat, 
                           data.dists = mydists,
                           method = "bayes",max.parents = max.par) 
mp.dag <- mostProbable(score.cache = mycache)
myfit <- fitAbn(object = mp.dag)
plot(myfit)
```

We would like to compute the conditional expectation $\mathbb{E}(g2|b2="yes",b5="yes")$.

Some useful definitions:

- **Hypothesis**: the hypothesis refers to the set of variables whose values we want to infer ($g2$ in the example above).

- **Evidence**: evidence refers to the set of known or observed values of other variables in the network ($b2$ and $b5$ in the example above). 

## Methodology

The inference procedure we propose is an iterative process, which main goal consists in refining the estimate of the hypothesis based on both its parents and children:

1. Estimate the hypothesis using the parents, $\mathbb{E}(g2|b2 = "yes")$ in our example,

2. Update the estimate by considering the children, $\mathbb{E}(g2|b5="yes")$ in our example is updated based on the initial estimate (prior) from Step 1,

3. Update the estimate by averaging for a more balanced estimate, $\mathbb{E}(g2|b2 = "yes",b5="yes") = (1/2)\times \mathbb{E}(g2|b2 = "yes") + (1/2) \times \mathbb{E}(g2|b5 = "yes").$

4. Iterate until convergence: the updated estimate from Step 3 becomes the priors for the next step (Step 2). 

### Step 1: estimation using the parents

Let's generalize by considering that the hypothesis $X$ (to estimate) depends on $G$, a Gaussian node, $P$, a Poisson node and $B$ a binomial node. We would like to compute $\mathbb{E}(X|G,P,B)$.

```{r diagram, echo=FALSE}
library(DiagrammeR)
grViz("
digraph G {
  graph [layout = dot]
  G -> X
  P -> X
  B -> X
}
", width=300)
```

Depending on the cases:

- if $X$ is gaussian, then:
$\mathbb{E}(X|G,P,B) = \beta_{0} + \beta_{1} G +  \beta_{2} P + \beta_{3} B.$

- if $X$ is Poisson, then:
$\mathbb{E}(X|G,P,B) = \exp \left( \beta_{0} + \beta_{1} G +  \beta_{2} P + \beta_{3} B \right).$

- if $X$ is binomial, then:
$\mathbb{P}(X=1|G,P,B) = \frac{1}{1+\exp \left( - (\beta_{0} + \beta_{1} G +  \beta_{2} P + \beta_{3} B)\right)}$ and $\mathbb{P}(X=0|G,P,B)=1-\mathbb{P}(X=1|G,P,B).$ 

To compute these quantities:

- If all parents ($G$, $P$ and $B$) are known (evidence), substitute their values directly into the formula to compute the desired conditional expectation or probability.

- If at least one of the parents is an evidence and one of the parents is not an evidence, 

  - for the evidence parents, substitute their values directly into the formula
  
  - for the unknown parents, estimate their values using the observed mean from the data (for gaussian or Poisson variables) or the observed frequencies (for binomial variables) and then substitute the estimated values into the formula.

- If none of the parents are evidence, then $\mathbb{E}(X|G,P,B)$ is estimated using the data directly (observed mean for gaussian or Poisson, observed frequencies for binomial).

In our example, the algorithm works as follows:
```{r, message=FALSE}
library(igraph)
source("PredictABN_v2.R")

hypothesis <- "g2" 
evidence <- list("b2"="y","b5"="y") 
graph <- graph_from_adjacency_matrix(t(mp.dag$dag))

# Check if the evidence have the right format
evidence <- check_evidence(data = mydat, graph, mydists, hypothesis, evidence)

# Find the parents of the hypothesis
parents <- find_parents(graph, hypothesis)

# Estimate E(g2|b2=yes)
prediction <- predict_node(data = mydat, mydists, hypothesis, evidence, parents, myfit = myfit$modes)
print(paste0("The expected value E(g2|b2=yes) is equal to ",round(prediction[1],2)))
```

Note that to fully determine the distribution of $X$ with $X$ Gaussian, we need to estimate its variance $\hat{\color{white}{\sigma}}\sigma ^2(X)$. This is done using the well-known formula:
$$\hat{\color{white}{\sigma}}\sigma ^2(X)= \frac{\sum_i \hat{\color{white}{\sigma}}\varepsilon_i^2}{n-p},$$
where $\hat{\color{white}{\sigma}}\varepsilon_i$ represents the residuals, calculated as $\hat{\color{white}{\sigma}}\varepsilon_i = X_i - (\beta_0 + \beta_1 G_i + \beta_2 P_i + \beta_3 B_i)$, $n$ is the total number of observations, and $p$ is the number of variables in the model (in this case 3).

```{r, message=FALSE}
print(paste0("The estimated variance var(g2|b2=yes) is ",round(prediction[2],2)))
```

### Step 2: estimate's update using the children

Let's now consider that the hypothesis $X$ (to estimate) influences $G_2$, a Gaussian node, $P_2$, a Poisson node and $B_2$ a binomial node. We would like to compute $\mathbb{E}(X|G_2,P_2,B_2)$ based on the initial estimate (prior) from Step 1.

```{r diagram2, echo=FALSE}
library(DiagrammeR)
grViz("
digraph G {
  graph [layout = dot]
  G1 -> X
  P1 -> X
  B1 -> X
  X -> G2
  X -> P2
  X -> B2
}
", width=300)
```

Let's begin by computing $\mathbb{E}(X|Y)$ with $Y$ being either $G_2$, $P_2$ or $B_2$. 
Depending on the cases:

- If $X$ is binomial, then Bayes theorem gives:
$$ \mathbb{P}(X|Y) = \frac{\mathbb{P}(Y|X) \mathbb{P}(X)}{\mathbb{P}(Y)}= \frac{\mathbb{P}(Y|X) \mathbb{P}(X)}{\sum_X \mathbb{P}(Y|X) \mathbb{P}(X)}.$$
- If $X$ is gaussian or Poisson, then the same holds for $\mathbb{E}(X|Y)$:
$$\mathbb{E}(X|Y) = \int X \mathbb{P}(X|Y) dX = \int X \frac{\mathbb{P}(Y|X) \mathbb{P}(X)}{\mathbb{P}(Y)} dX= \frac{\int X \mathbb{P}(Y|X) \mathbb{P}(X) dX}{\int \mathbb{P}(Y|X) \mathbb{P}(X) dX}.$$

We will use the estimates $\mathbb{E}(X|G_1,P_1,B_1)$ of Step 1 as a prior (for $\mathbb{P}(X)$):

- if $X$ is binomial, $X \sim B(p_X)$ with $p_X$ as estimated in Step 1.

- if $X$ is gaussian, $X \sim N(\mu_X,\sigma_X^2)$ with $\mu_X$ and $\sigma_X^2$ as estimated in Step 1.

- if $X$ is Poisson, $X \sim P(\lambda_X)$ with $\lambda_X$ as estimated in Step 1.

The conditional probability $\mathbb{P}(Y|X)$ is derived from the likelihood $L(Y|X)$:

  - if $Y$ is binomial, $Y \sim B(p_Y)$ with $p_Y = \frac{1}{1+\exp \left( - (\beta_{Y0} + \beta_{Y1} X)\right)}.$
  
  - if $Y$ is gaussian, $Y \sim N(\mu_Y,\sigma_Y^2)$ with $\mu_Y = \beta_{Y0} + \beta_{Y1} X$.
  
  - if $Y$ is Poisson, $Y \sim P(\lambda_Y)$ with $\lambda_Y = \exp \left( \beta_{Y0} + \beta_{Y1} X \right)$.
  
Now, depending on the cases:

- If none of the children are evidence, forget this update step.

- If only one child is an evidence (e.g. $G_2$), then $\mathbb{E}(X|G_2,P_2,B_2)$ will be estimated using this node only (e.g. $\mathbb{E}(X|G_2)$).

- If more than one child is an evidence, then the expectations will be averaged: $\mathbb{E}(X|G_2,P_2,B_2)=(1/3)\times \mathbb{E}(X|G_2) + (1/3)\times \mathbb{E}(X|P_2)+(1/3)\times \mathbb{E}(X|B_2).$

In our example, the algorithm works as follows.

```{r}
# Find the children of the hypothesis
children <- find_children(graph, hypothesis)

# Update the estimated value using the children
prediction_updated <- update_predicted_node(data = mydat, graph, mydists, hypothesis, evidence, children, myfit = myfit$modes, prediction)

print(paste0("The expected value E(g2|b5=yes) is equal to ",round(prediction_updated[1],2)))
```

### Step 3 and 4: Iterations until convergence

The estimate $\mathbb{E}(g2|b2 = "yes" | b5="yes)$ is updated by averaging the two estimates from Step 1 and 2 for a more balanced estimate:
$\mathbb{E}(g2|b2 = "yes", b5="yes)= (1/2) \times \mathbb{E}(g2|b2 = "yes") + (1/2) \times \mathbb{E}(g2| b5="yes").$

We then iterate until convergence: the updated estimate above becomes the priors for the next step (Step 2).

```{r}
results <- predictABN(data = mydat, mydists, dag = mp.dag$dag, myfit = myfit$modes, hypothesis, evidence)
print(paste0("The expected value E(g2|b2=yes,b5=yes) is equal to ",round(results[1],2)))
```

