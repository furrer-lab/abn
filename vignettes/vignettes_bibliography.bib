
@article{koivisto_exact_2004,
	title = {Exact {Bayesian} {Structure} {Discovery} in {Bayesian} {Networks}},
	abstract = {Learning a Bayesian network structure from data is a well-motivated but computationally hard task. We present an algorithm that computes the exact posterior probability of a subnetwork, e.g., a directed edge; a modiﬁed version of the algorithm ﬁnds one of the most probable network structures. This algorithm runs in time O(n2n + nk+1C(m)), where n is the number of network variables, k is a constant maximum in-degree, and C(m) is the cost of computing a single local marginal conditional likelihood for m data instances. This is the ﬁrst algorithm with less than super-exponential complexity with respect to n. Exact computation allows us to tackle complex cases where existing Monte Carlo methods and local search procedures potentially fail. We show that also in domains with a large number of variables, exact computation is feasible, given suitable a priori restrictions on the structures; combining exact and inexact methods is also possible. We demonstrate the applicability of the presented algorithm on four synthetic data sets with 17, 22, 37, and 100 variables.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Koivisto, Mikko and Sood, Kismat},
	year = {2004},
	pages = {25},
	file = {Koivisto and Sood - Exact Bayesian Structure Discovery in Bayesian Net.pdf:/home/matteo/Zotero/storage/RAJWGQVE/Koivisto and Sood - Exact Bayesian Structure Discovery in Bayesian Net.pdf:application/pdf},
}

@book{koller_probabilistic_2009,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {978-0-262-01319-2},
	shorttitle = {Probabilistic graphical models},
	language = {en},
	publisher = {MIT Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
	keywords = {Bayesian statistical decision theory, Graphic methods, Graphical modeling (Statistics)},
	file = {Koller and Friedman - 2009 - Probabilistic graphical models principles and tec.pdf:/home/matteo/Zotero/storage/SX88LGET/Koller and Friedman - 2009 - Probabilistic graphical models principles and tec.pdf:application/pdf},
}

@article{madigan_bayesian_1995,
	title = {Bayesian {Graphical} {Models} for {Discrete} {Data}},
	volume = {63},
	issn = {03067734},
	url = {https://www.jstor.org/stable/1403615?origin=crossref},
	doi = {10.2307/1403615},
	number = {2},
	urldate = {2021-10-21},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Madigan, David and York, Jeremy and Allard, Denis},
	month = aug,
	year = {1995},
	pages = {215},
	file = {Submitted Version:/home/matteo/Zotero/storage/NV8LY54W/Madigan et al. - 1995 - Bayesian Graphical Models for Discrete Data.pdf:application/pdf;Submitted Version:/home/matteo/Zotero/storage/EXHPALDF/Madigan et al. - 1995 - Bayesian Graphical Models for Discrete Data.pdf:application/pdf},
}

@article{paolo_giudici_improving_2003,
	title = {Improving {Markov} {Chain} {Monte} {Carlo} {Model} {Search} for {Data} {Mining}},
	volume = {50},
	abstract = {The motivation of this paper is the application of MCMC model scoring procedures to data mining problems, involving a large number of competing models and other relevant model choice aspects. To achieve this aim we analyze one of the most popular Markov Chain Monte Carlo methods for structural learning in graphical models, namely, the MC3 algorithm proposed by D. Madigan and J. York (International Statistical Review, 63, 215–232, 1995). Our aim is to improve their algorithm to make it an effective and reliable tool in the ﬁeld of data mining. In such context, typically highly dimensional in the number of variables, little can be known a priori and, therefore, a good model search algorithm is crucial.},
	language = {en},
	journal = {Machine Learning},
	author = {{Paolo Giudici} and {Roberto Castello}},
	year = {2003},
	pages = {32},
	file = {Giudici - Improving Markov Chain Monte Carlo Model Search fo.pdf:/home/matteo/Zotero/storage/SA4D4A97/Giudici - Improving Markov Chain Monte Carlo Model Search fo.pdf:application/pdf},
}

@article{heckerman_learning_1995,
	title = {Learning {Bayesian} {Networks}: {The} {Combination} of {Knowledge} and {Statistical} {Data}},
	volume = {20},
	issn = {1573-0565},
	shorttitle = {Learning {Bayesian} {Networks}},
	url = {https://doi.org/10.1023/A:1022623210503},
	doi = {10.1023/A:1022623210503},
	abstract = {We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user's priors for network parameters can be encoded in a single Bayesian network for the next case to be seen—a prior network—and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k = 1 parent. For the general case (k {\textgreater} 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.},
	language = {en},
	number = {3},
	urldate = {2024-02-15},
	journal = {Machine Learning},
	author = {Heckerman, David and Geiger, Dan and Chickering, David M.},
	month = sep,
	year = {1995},
	keywords = {Bayesian networks, Dirichlet, heuristic search, learning, likelihood equivalence, maximum branching},
	pages = {197--243},
	file = {Full Text PDF:/home/matteo/Zotero/storage/9HXBJDP5/Heckerman et al. - 1995 - Learning Bayesian Networks The Combination of Kno.pdf:application/pdf},
}

@article{metropolis_equation_1953,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	volume = {21},
	issn = {0021-9606},
	url = {https://doi.org/10.1063/1.1699114},
	doi = {10.1063/1.1699114},
	abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
	number = {6},
	urldate = {2024-02-15},
	journal = {The Journal of Chemical Physics},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	month = jun,
	year = {1953},
	pages = {1087--1092},
	file = {Full Text PDF:/home/matteo/Zotero/storage/R6NYN8WG/Metropolis et al. - 1953 - Equation of state calculations by fast computing m.pdf:application/pdf;Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf:/home/matteo/Zotero/storage/6YD94EBG/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf:application/pdf;Snapshot:/home/matteo/Zotero/storage/5WDPLVJY/Equation-of-State-Calculations-by-Fast-Computing.html:text/html},
}
